{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b704b660",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e4df0e5f",
   "metadata": {},
   "source": [
    "## library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1c95ee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:40.283723Z",
     "start_time": "2023-05-31T04:34:37.938576Z"
    }
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import operator\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "device_lib.list_local_devices()\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from pandas.tseries.offsets import DateOffset\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.dates as mdates\n",
    "from IPython.display import set_matplotlib_formats\n",
    "\n",
    "sns.set(style='white', context='notebook', palette='deep')\n",
    "line_color = ['#FFBF00','#FF7F50','#DE3163','#9FE2BF','#40E0D0','#6495ED','#117A65','#2471A3','#CCCCFF','#8E44AD','#CD5C5C' ,'#F08080','#FA8072' ,'#E9967A' ,'#FFA07A']\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.style.use(\"seaborn-white\")\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "matplotlib.rcParams['axes.unicode_minus'] = False\n",
    "#print(plt.rcParams['font.family'])\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0ce72b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:43.538269Z",
     "start_time": "2023-05-31T04:34:42.750463Z"
    }
   },
   "outputs": [],
   "source": [
    "#? 파이프라인\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "#? scaler\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "#? 통계\n",
    "#import statsmodels.api as sm\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from scipy.stats import mstats\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#? 평가지표\n",
    "import hydroeval as he\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#?모델 \n",
    "import lazypredict\n",
    "from lazypredict.Supervised import LazyRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#?모델링 \n",
    "import joblib\n",
    "import optuna\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.inspection import permutation_importance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c614a205",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ca76fc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:45.105437Z",
     "start_time": "2023-05-31T04:34:45.089490Z"
    }
   },
   "outputs": [],
   "source": [
    "# 예측과 실제 수위를 scatter plot해주는 함수 \n",
    "def scatter_plot(pred,answer):\n",
    "    x = pred\n",
    "    y = answer\n",
    "\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(7, 7))\n",
    "    rmse,nse,r2=metric(y,x)\n",
    "    axes.scatter(x, y, label='data') \n",
    "    lims = [np.min([axes.get_xlim(), axes.get_ylim()]), np.max([axes.get_xlim(), axes.get_ylim()]), ]\n",
    "    axes.plot(lims, lims, 'k--', alpha=0.75, zorder=0, label='parity')\n",
    "    axes.set_aspect('equal')\n",
    "    axes.set_xlabel('Prediction',fontsize=25)\n",
    "    axes.set_ylabel('Observation',fontsize=25)\n",
    "    handles, labels = axes.get_legend_handles_labels()\n",
    "    txt1=\"(a)   Jamsu bridge RMSE %.4f\"%rmse\n",
    "    axes.set_title(txt1, fontsize=25,loc='left')\n",
    "    axes.xaxis.set_tick_params(labelsize=20)\n",
    "    axes.yaxis.set_tick_params(labelsize=20)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255766ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:45.859062Z",
     "start_time": "2023-05-31T04:34:45.854148Z"
    }
   },
   "outputs": [],
   "source": [
    "# 파일이 존재하는지 확인하는 함수 \n",
    "def check(filepath):\n",
    "    csv_files = glob(os.path.join(filepath, \"*.csv\"))\n",
    "    if len(csv_files) > 0:return 1\n",
    "    else:return 0\n",
    "    \n",
    "# 그래프에 rmse를 표시해주는 함수 \n",
    "def plot_rmse(ax, answer, preds, label):\n",
    "        ax.text(1.0, 0.95, '  RMSE: {:.3f}  '.format(metric(answer,preds)[0]),\n",
    "                fontsize=80, ha='right', va='top', transform=ax.transAxes)\n",
    "        \n",
    "# rmse와 nse를 계산해주는 함수(m단위)\n",
    "def metric(y_true, y_pred):\n",
    "    y_true=y_true/100\n",
    "    y_pred=y_pred/100\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    nse=he.evaluator(he.nse, y_pred, y_true)\n",
    "    return rmse,nse,r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aaf87b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:47.170463Z",
     "start_time": "2023-05-31T04:34:47.162491Z"
    }
   },
   "outputs": [],
   "source": [
    "# 선행시간, 이동평균, 윈도우에 맞게 데이터셋을 제공 \n",
    "def load_dataset2(leadtime,moving_average,version):\n",
    "    \n",
    "    # 이동평균을 적용할 feature들 \n",
    "    select_features=['cd_br','hj_br','jn_br','tl_gh_br','flow','water','wl_js_br']\n",
    "    # feature engineering 할 유량들.\n",
    "    fe_list=['cd_br','hj_br','jn_br']\n",
    "    \n",
    "    x=pd.concat([train_data,test_data],axis=0)\n",
    "    x.reset_index(drop=True,inplace=True)\n",
    "    x=x.set_index('ymdhm')\n",
    "    x.index=pd.to_datetime(x.index)\n",
    "    \n",
    "    \n",
    "    # 선행시간을 적용하기 위해 타겟을미뤄줌 \n",
    "    y=x['target'].shift(-leadtime)\n",
    "    x.drop('target',axis=1,inplace=True)\n",
    "    \n",
    "    # feature engineer을 안함(유량정보없음)\n",
    "    if(version==0):\n",
    "        x.drop(['fw_cd_br','fw_hj_br','fw_jn_br'],axis=1,inplace=True)\n",
    "    # feature engineering 함(유량정보있고, 팔당댐도 엔지니어링)\n",
    "    if(version==1):\n",
    "        # feature engineering한 버전\n",
    "        # x['otfinf']=x['tototf_pd_dam']-x['inf_pd_dam']\n",
    "        x.drop([ 'ecpc_pd_dam'],axis=1,inplace=True)\n",
    "        \n",
    "        # 월과 시간에 대한 feature도 추가해줌 \n",
    "        x['month'],x['hour']=x.index.month,x.index.hour\n",
    "        \n",
    "        #! 유량 컬럼의 상관관계가 높아, \n",
    "        #! 이전시간과의 유량차이 column을 추가 \n",
    "        \n",
    "        flow = PCA(n_components=1)\n",
    "        flow.fit(x[['tototf_pd_dam','inf_pd_dam']])\n",
    "        transformed_data = flow.transform(x[['tototf_pd_dam','inf_pd_dam']])  # 변환된 데이터\n",
    "        x['flow']=transformed_data\n",
    "        \n",
    "        water = PCA(n_components=1)\n",
    "        water.fit(x[['sfw_pd_dam','wl_pd_dam']])\n",
    "        transformed_data = water.transform(x[['sfw_pd_dam','wl_pd_dam']])  # 변환된 데이터\n",
    "        x['water']=transformed_data\n",
    "        \n",
    "        x.drop(['tototf_pd_dam','inf_pd_dam','sfw_pd_dam','wl_pd_dam'],axis=1,inplace=True)\n",
    "        \n",
    "        fe_list=['cd_br','hj_br','jn_br']\n",
    "        for i in fe_list:\n",
    "            v_name = f\"{i}_pca\"  # 동적으로 생성할 변수명\n",
    "            f_name,w_name = \"fw_\"+i, 'wl_'+i\n",
    "            tmp=x[[f_name,w_name]]\n",
    "            globals()[v_name] = PCA(n_components=1)  # 변수 생성\n",
    "            globals()[v_name].fit(tmp)  # PCA 수행\n",
    "            transformed_data = globals()[v_name].transform(tmp)  # 변환된 데이터\n",
    "            x[i]=transformed_data\n",
    "            x.drop([f_name],axis=1,inplace=True)\n",
    "            if(moving_average<1): x.drop([w_name],axis=1,inplace=True)\n",
    "       \n",
    "    \n",
    "    # 이동평균을 적용\n",
    "    if(moving_average>1):\n",
    "        for i in range(len(select_features)):\n",
    "            coln=select_features[i]+str(moving_average)+'ma'\n",
    "            x[coln] = x[select_features[i]].rolling(window=moving_average).mean()\n",
    "            if(i<3):x.drop('wl_'+select_features[i],axis=1,inplace=True)\n",
    "            \n",
    "        \n",
    "    # train과 test를 다시 분리       \n",
    "    idx = x.index.get_loc('2022-06-21 00:00:00')\n",
    "    \n",
    "    # test를 위해 train과 test의 범위를 선행시간만큼 조정 \n",
    "    x_train=x[:idx-leadtime]\n",
    "    x_test=x[idx-leadtime:]\n",
    "    y_train = y[:idx - leadtime]\n",
    "    \n",
    "    if(version==1 and moving_average==0):\n",
    "        x_train=x_train[1:]\n",
    "        y_train=y_train[1:]\n",
    "\n",
    "    # 이동평균을 적용하면 생기는 nan을 없애주기 위함 \n",
    "    if(moving_average!=0):\n",
    "        x_train=x_train[moving_average:]\n",
    "        y_train=y_train[moving_average:]\n",
    "        \n",
    "    dataset=[x_train,x_test,y_train]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d73c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선행시간, 이동평균, 윈도우에 맞게 데이터셋을 제공 \n",
    "def load_dataset(leadtime,moving_average,version):\n",
    "    \n",
    "    # 이동평균을 적용할 feature들 \n",
    "    select_features=['wl_js_br','wl_cd_br','wl_hj_br','wl_jn_br','wl_pd_dam','tl_gh_br']\n",
    "    # feature engineering 할 유량들.\n",
    "    fe_list=['cd_br','hj_br','jn_br']\n",
    "    \n",
    "    x=pd.concat([train_data,test_data],axis=0)\n",
    "    x.reset_index(drop=True,inplace=True)\n",
    "    x=x.set_index('ymdhm')\n",
    "    x.index=pd.to_datetime(x.index)\n",
    "    \n",
    "    # 선행시간을 적용하기 위해 타겟을미뤄줌 \n",
    "    y=x['target'].shift(-leadtime)\n",
    "    x.drop('target',axis=1,inplace=True)\n",
    "    \n",
    "    # feature engineer을 안함(유량정보없음)\n",
    "    if(version==0):\n",
    "        x.drop(['fw_cd_br','fw_hj_br','fw_jn_br'],axis=1,inplace=True)\n",
    "    # feature engineering 함(유량정보있고, 팔당댐도 엔지니어링)\n",
    "    if(version==1):\n",
    "        # feature engineering한 버전\n",
    "        x['otfinf']=x['tototf_pd_dam']-x['inf_pd_dam']\n",
    "        x.drop([ 'inf_pd_dam','ecpc_pd_dam', 'tototf_pd_dam'],axis=1,inplace=True)\n",
    "        \n",
    "        # 월과 시간에 대한 feature도 추가해줌 \n",
    "        x['month'],x['hour']=x.index.month,x.index.hour\n",
    "        \n",
    "        #! 유량 컬럼의 상관관계가 높아, \n",
    "        #! 이전시간과의 유량차이 column을 추가 \n",
    "        for i in fe_list:\n",
    "            name1='fw_'+i\n",
    "            name2='fw_'+i+'_lag'\n",
    "\n",
    "            x[name2]= x[name1] - x[name1].shift(1)\n",
    "            x.drop(name1,axis=1,inplace=True)\n",
    "            \n",
    "    # 이동평균을 적용\n",
    "    if(moving_average>1):\n",
    "        for i in range(len(select_features)):\n",
    "            coln=select_features[i]+str(moving_average)+'ma'\n",
    "            x[coln] = x[select_features[i]].rolling(window=moving_average).mean()\n",
    "        \n",
    "    # train과 test를 다시 분리       \n",
    "    idx = x.index.get_loc('2022-06-21 00:00:00')\n",
    "    \n",
    "    # test를 위해 train과 test의 범위를 선행시간만큼 조정 \n",
    "    x_train=x[:idx-leadtime]\n",
    "    x_test=x[idx-leadtime:]\n",
    "    y_train = y[:idx - leadtime]\n",
    "    \n",
    "    if(version==1 and moving_average==0):\n",
    "        x_train=x_train[1:]\n",
    "        y_train=y_train[1:]\n",
    "\n",
    "    # 이동평균을 적용하면 생기는 nan을 없애주기 위함 \n",
    "    if(moving_average!=0):\n",
    "        x_train=x_train[moving_average:]\n",
    "        y_train=y_train[moving_average:]\n",
    "        \n",
    "    dataset=[x_train,x_test,y_train]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b394cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선행시간, 이동평균, 윈도우에 맞게 데이터셋을 제공 \n",
    "def load_dataset3(leadtime,moving_average,version):\n",
    "    \n",
    "    # 이동평균을 적용할 feature들 \n",
    "    select_features=['wl_js_br','wl_cd_br','wl_hj_br','wl_jn_br','wl_pd_dam','tl_gh_br']\n",
    "    # feature engineering 할 유량들.\n",
    "    fe_list=['cd_br','hj_br','jn_br']\n",
    "    \n",
    "    x=pd.concat([train_data,test_data],axis=0)\n",
    "    x.reset_index(drop=True,inplace=True)\n",
    "    x=x.set_index('ymdhm')\n",
    "    x.index=pd.to_datetime(x.index)\n",
    "    \n",
    "    # 선행시간을 적용하기 위해 타겟을미뤄줌 \n",
    "    y=x['target'].shift(-leadtime)\n",
    "    x.drop('target',axis=1,inplace=True)\n",
    "    \n",
    "    # feature engineer을 안함(유량정보없음)\n",
    "    if(version==0):\n",
    "        x.drop(['fw_cd_br','fw_hj_br','fw_jn_br'],axis=1,inplace=True)\n",
    "    # feature engineering 함(유량정보있고, 팔당댐도 엔지니어링)\n",
    "    if(version==1):\n",
    "        # feature engineering한 버전\n",
    "        x['otfinf']=x['tototf_pd_dam']-x['inf_pd_dam']\n",
    "        x.drop([ 'inf_pd_dam','ecpc_pd_dam', 'tototf_pd_dam'],axis=1,inplace=True)\n",
    "        \n",
    "        # 월과 시간에 대한 feature도 추가해줌 \n",
    "        x['month'],x['hour']=x.index.month,x.index.hour\n",
    "        \n",
    "        #! 유량 컬럼의 상관관계가 높아, \n",
    "        #! 이전시간과의 유량차이 column을 추가 \n",
    "    # 이동평균을 적용\n",
    "    if(moving_average>1):\n",
    "        for i in range(len(select_features)):\n",
    "            coln=select_features[i]+str(moving_average)+'ma'\n",
    "            x[coln] = x[select_features[i]].rolling(window=moving_average).mean()\n",
    "        \n",
    "    # train과 test를 다시 분리       \n",
    "    idx = x.index.get_loc('2022-06-21 00:00:00')\n",
    "    \n",
    "    # test를 위해 train과 test의 범위를 선행시간만큼 조정 \n",
    "    x_train=x[:idx-leadtime]\n",
    "    x_test=x[idx-leadtime:]\n",
    "    y_train = y[:idx - leadtime]\n",
    "    \n",
    "    if(version==1 and moving_average==0):\n",
    "        x_train=x_train[1:]\n",
    "        y_train=y_train[1:]\n",
    "\n",
    "    # 이동평균을 적용하면 생기는 nan을 없애주기 위함 \n",
    "    if(moving_average!=0):\n",
    "        x_train=x_train[moving_average:]\n",
    "        y_train=y_train[moving_average:]\n",
    "        \n",
    "    dataset=[x_train,x_test,y_train]\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a17b3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:47.849243Z",
     "start_time": "2023-05-31T04:34:47.834006Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_model(filepath,epoch,base_set):\n",
    "    \n",
    "    print(\"Modeling Start*******\")\n",
    "    # optuna 학습 \n",
    "    study_model = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "    \n",
    "    # early-stopping callback 추가\n",
    "    early_stopping = EarlyStoppingCallback(15, direction='minimize')\n",
    "    study_model.optimize(objective_randomforest, epoch, callbacks=[early_stopping])\n",
    "    \n",
    "    # optuna 학습결과 파라미터 가져오기\n",
    "    trial = study_model.best_trial\n",
    "    trial_params = trial.params\n",
    "    print(\" Value: \", trial.value)\n",
    "    print(\" Params: \")\n",
    "    for key, value in trial_params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "        \n",
    "    # optuna로 학습된 파라미터로 모델 재 학습 \n",
    "    model = RandomForestRegressor(**trial_params,random_state=624,) # 최적화된 파라미터를 통해 학습 \n",
    "    model.fit(base_set[0], base_set[2]) \n",
    "    pred = model.predict(base_set[1])\n",
    "    pred=pd.DataFrame(pred,columns=['wl_js_br'])\n",
    "    pred = pred[:-1] # 선행시간 10분에 따라 미뤄줌 \n",
    "    \n",
    "    return pred,study_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab20bb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:48.541819Z",
     "start_time": "2023-05-31T04:34:48.531850Z"
    }
   },
   "outputs": [],
   "source": [
    "sampler = TPESampler(seed=10)\n",
    "\n",
    "def objective_randomforest(trial):\n",
    "    \n",
    "    rf_param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 150),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 2, 60),\n",
    "        'n_jobs':-1\n",
    "    }\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(base_set[0], base_set[2], test_size=0.3, random_state=624)\n",
    "    \n",
    "    model = RandomForestRegressor(random_state=624, **rf_param)\n",
    "    model.fit(X_train,y_train)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_valid, model.predict(X_valid)))\n",
    "    \n",
    "    return rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad5947e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:49.130760Z",
     "start_time": "2023-05-31T04:34:49.118064Z"
    }
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(object):\n",
    "    \n",
    "    # 평가지수가 감소하는 방향으로 학습하면 minimize\n",
    "    # 평가지수가 증가하는 방향으로 학습하면 maximize\n",
    "\n",
    "    def __init__(self, early_stopping_rounds: int, direction: str = \"minimize\") -> None:\n",
    "        self.early_stopping_rounds = early_stopping_rounds\n",
    "\n",
    "        self._iter = 0\n",
    "\n",
    "        if direction == \"minimize\":\n",
    "            self._operator = operator.lt\n",
    "            self._score = np.inf\n",
    "        elif direction == \"maximize\":\n",
    "            self._operator = operator.gt\n",
    "            self._score = -np.inf\n",
    "        else:\n",
    "            ValueError(f\"invalid direction: {direction}\")\n",
    "\n",
    "    def __call__(self, study: optuna.Study, trial: optuna.Trial) -> None:\n",
    "        \"\"\"Do early stopping.\"\"\"\n",
    "        if self._operator(study.best_value, self._score):\n",
    "            self._iter = 0\n",
    "            self._score = study.best_value\n",
    "        else:\n",
    "            self._iter += 1\n",
    "\n",
    "        if self._iter >= self.early_stopping_rounds:\n",
    "            study.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f7c701c9",
   "metadata": {},
   "source": [
    "# Data Load and Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2b6c022e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:29:42.038324Z",
     "start_time": "2023-05-15T08:29:42.031867Z"
    }
   },
   "source": [
    "## Origin data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fc8322",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T14:11:13.329243Z",
     "start_time": "2023-05-19T14:11:11.945607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/full_data.csv\")\n",
    "train_data=pd.read_csv(\"../data/train_data.csv\")\n",
    "test_data=pd.read_csv(\"../data/test_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78186940",
   "metadata": {},
   "source": [
    "## data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3764192",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T14:12:28.565047Z",
     "start_time": "2023-05-19T14:12:27.989179Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.set_index(['ymdhm'],drop=True)\n",
    "train_data.index=pd.to_datetime(train_data.index)\n",
    "train_data=train_data.replace('##########',np.nan)\n",
    "train_data=train_data.replace(' ',np.nan)\n",
    "train_data=train_data.astype('float')\n",
    "train_data.drop(['fw_js_br'],axis=1,inplace=True)\n",
    "train_data.drop(['fw_gj_br'],axis=1,inplace=True)\n",
    "\n",
    "test_data = test_data.set_index(['ymdhm'],drop=True)\n",
    "test_data.index=pd.to_datetime(test_data.index)\n",
    "test_data=test_data.replace('##########',np.nan)\n",
    "test_data=test_data.replace(' ',np.nan)\n",
    "test_data=test_data.astype('float')\n",
    "test_data.drop(['fw_js_br'],axis=1,inplace=True)\n",
    "test_data.drop(['fw_gj_br'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f7c1c6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T14:12:35.303687Z",
     "start_time": "2023-05-19T14:12:34.845455Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_list=['pr_jg','pr_dg','pr_sj']\n",
    "wl_list=['wl_js_br','wl_cd_br','wl_hj_br','wl_hg_br','wl_gj_br','wl_pd_br','wl_jn_br']\n",
    "\n",
    "for col in train_data.columns:\n",
    "    if(col not in rf_list):\n",
    "        q1 = train_data[col].quantile(0.005)\n",
    "        train_data[col][train_data[col]<q1]=np.nan\n",
    "        if(col not in wl_list):\n",
    "            q99 = train_data[col].quantile(0.995)\n",
    "            train_data[col][train_data[col]>q99]=np.nan\n",
    "\n",
    "    train_data[col] = train_data[col].interpolate(method='time')\n",
    "train_data['wl_gj_br'] = mstats.winsorize(train_data['wl_gj_br'], limits=[0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a536552",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T14:14:06.156120Z",
     "start_time": "2023-05-19T14:13:03.676871Z"
    }
   },
   "outputs": [],
   "source": [
    "rf_list=['pr_jg','pr_dg','pr_sj']\n",
    "wl_list=['wl_js_br','wl_cd_br','wl_hj_br','wl_hg_br','wl_gj_br','wl_pd_br','wl_jn_br']\n",
    "\n",
    "for col in test_data.columns:\n",
    "    if(col not in rf_list):\n",
    "        q1 = test_data[col].quantile(0.005)\n",
    "        test_data[col][test_data[col]<q1]=np.nan\n",
    "        if(col not in wl_list):\n",
    "            q99 = test_data[col].quantile(0.995)\n",
    "            test_data[col][test_data[col]>q99]=np.nan\n",
    "test_data['wl_gj_br'] = mstats.winsorize(test_data['wl_gj_br'], limits=[0.01, 0.01])\n",
    "\n",
    "data=pd.concat([train_data,test_data],axis=0)\n",
    "\n",
    "idx = data.loc[data.isna().any(axis=1)].index\n",
    "idx=idx.unique()\n",
    "for i in tqdm(idx):\n",
    "    tmp=data[:i]\n",
    "    tmp=tmp.interpolate(method='time')\n",
    "    for j in data.columns:\n",
    "        if pd.isna(data.loc[i, j]):\n",
    "            data.loc[i][j]=tmp.iloc[-1][j]\n",
    "idx = data.index.get_loc('2022-06-21 00:00:00')\n",
    "test_data=data[idx:]\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054a285a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T14:17:27.715296Z",
     "start_time": "2023-05-19T14:17:27.659357Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data=train_data[['wl_js_br', 'wl_cd_br', 'wl_hj_br','wl_hg_br', 'wl_gj_br','wl_pd_br','wl_jn_br',\n",
    "                       'fw_cd_br', 'fw_hj_br', 'fw_hg_br', 'fw_pd_br','fw_jn_br',  \n",
    "                       'wl_pd_dam','inf_pd_dam', 'sfw_pd_dam', 'ecpc_pd_dam', 'tototf_pd_dam', \n",
    "                       'pr_jg','pr_dg', 'pr_sj', 'tl_gh_br', ]]\n",
    "test_data=test_data[['wl_js_br', 'wl_cd_br', 'wl_hj_br','wl_hg_br', 'wl_gj_br','wl_pd_br','wl_jn_br',\n",
    "                       'fw_cd_br', 'fw_hj_br', 'fw_hg_br', 'fw_pd_br','fw_jn_br',  \n",
    "                       'wl_pd_dam','inf_pd_dam', 'sfw_pd_dam', 'ecpc_pd_dam', 'tototf_pd_dam', \n",
    "                       'pr_jg','pr_dg', 'pr_sj', 'tl_gh_br', ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3f0a49",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-19T14:18:30.710638Z",
     "start_time": "2023-05-19T14:18:26.503717Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.to_csv(\"data/refined_train_data.csv\",index=False)\n",
    "test_data.to_csv(\"data/refined_test_data.csv\",index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7fc49400",
   "metadata": {},
   "source": [
    "# Refined data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfdf47c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:34:54.340803Z",
     "start_time": "2023-05-31T04:34:53.192165Z"
    }
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"../data/new_Refined_data.csv\")\n",
    "train_data=pd.read_csv(\"../data/refined_train_data.csv\")\n",
    "test_data=pd.read_csv(\"../data/refined_test_data.csv\")\n",
    "answer=pd.read_csv(\"../data/answer.csv\")\n",
    "\n",
    "train_data['target']=train_data['wl_js_br']\n",
    "test_data['target']=0\n",
    "# answer\n",
    "\n",
    "train_data.drop(['wl_hg_br','fw_hg_br','wl_gj_br','wl_pd_br','fw_pd_br'],axis=1,inplace=True)\n",
    "test_data.drop(['wl_hg_br','fw_hg_br','wl_gj_br','wl_pd_br','fw_pd_br'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379f5754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 특정 독립 변수를 종속 변수로 선형 회귀 모델을 적합\n",
    "2. 이때 해당 독립 변수를 제외한 나머지 독립 변수들을 독립 변수로 사용하여 선형 회귀 모델을 다시 적합\n",
    "3. 두 번째 회귀 모델의 결정 계수(R-squared)를 구합니다.\n",
    "4. VIF 값은 1 / (1 - R-squared)으로 계산됩니다.\n",
    "\n",
    "* VIF < 1 : 다른 독립 변수들과의 상관 관계가 크지 않음 = 다중곤선성의 영향 적음.\n",
    "* VIF == 1 :독립 변수 간의 상관 관계가 약하거나 전혀 없음.\n",
    "* VIF > 1  :다른 독립 변수들과의 상관 관계가 강력 =  다중공선성의 영향 높음.\n",
    "\"\"\"\n",
    "\n",
    "df=load_dataset2(1, 6, 1)[0]\n",
    "#df=pd.concat([tmp[0],tmp[2]],axis=1)\n",
    "df['Intercept'] = 1\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df['Features'] = df.columns\n",
    "vif_df['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "vif_df\n",
    "high_vif_vars = vif_df[vif_df['VIF'] >= 10]['Features']\n",
    "low_vif_vars = vif_df[vif_df['VIF'] < 10]['Features']\n",
    "\n",
    "high_vif_vars=[i for i in high_vif_vars]\n",
    "\n",
    "print(\"Variables with VIF >= 10:\")\n",
    "print(high_vif_vars)\n",
    "\n",
    "print(\"Variables with VIF < 10:\")\n",
    "print(low_vif_vars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "319764d4",
   "metadata": {},
   "source": [
    "## Correlation heatmap"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e5b8ceb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T08:31:10.267617Z",
     "start_time": "2023-05-15T08:31:10.254279Z"
    }
   },
   "source": [
    "###  선행시간에 따른 target 과 나머지 feature간의 상관관계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12ace8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T03:36:47.200509Z",
     "start_time": "2023-05-31T03:36:42.095580Z"
    }
   },
   "outputs": [],
   "source": [
    "days_list=[1,3,6,9,12,15,18,24,30,36,48,60]\n",
    "df_list=[]\n",
    "\n",
    "for i in range(len(days_list)):\n",
    "    tmp=load_dataset(days_list[i], 6, 1)\n",
    "    df=pd.concat([tmp[0],tmp[2]],axis=1)\n",
    "    df=df[:497800] # 데이터의 갯수가 같도록 임의의 숫자 \n",
    "    corr = df.corrwith(df['target']).drop('target')\n",
    "    corr_df = pd.DataFrame(corr, columns=['correlation'])\n",
    "    corr_df.index.name = 'variables'\n",
    "    corr_df['dataset'] = int(days_list[i])\n",
    "    df_list.append(corr_df)\n",
    "final_corr=pd.concat(df_list)\n",
    "\n",
    "sns.set(font_scale=1.5)\n",
    "fig, ax = plt.subplots(figsize=(15, 15))\n",
    "cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "heatmap = sns.heatmap(final_corr.pivot_table(values='correlation', index='dataset', columns='variables'), \n",
    "                      vmax=1, linewidths=0.1, square=True, annot=True, cmap=cmap, vmin=-1,\n",
    "                      linecolor=\"white\", annot_kws={'size': 10},\n",
    "                      cbar_kws={'fraction': 0.05, 'pad': 0.03,'aspect': 11})\n",
    "\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=10,rotation=45)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=10,rotation=360)\n",
    "\n",
    "plt.show();\n",
    "#fig.savefig(f\"image/선행시간target히트맵\", transparent = True,dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2245deff",
   "metadata": {},
   "source": [
    "### correlation between all_data(all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35273bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T03:37:06.404372Z",
     "start_time": "2023-05-31T03:37:05.453791Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_set=load_dataset(1,0,0)[0]\n",
    "corr = base_set.corr()\n",
    "sns.set(font_scale=2)\n",
    "cols=[i for i in base_set.columns]\n",
    "fig, ax = plt.subplots(figsize=(35,35))\n",
    "cmap = sns.diverging_palette(240, 10, as_cmap=True)\n",
    "heatmap = sns.heatmap(corr, vmax=1, linewidths=0.1, square=True, annot=True, cmap=cmap, vmin=-1,\n",
    "            linecolor=\"white\",xticklabels = cols ,annot_kws = {'size':25},yticklabels = cols,\n",
    "            cbar_kws={'fraction':0.05, 'pad':0.03})\n",
    "\n",
    "heatmap.set_xticklabels(heatmap.get_xticklabels(), fontsize=30)\n",
    "heatmap.set_yticklabels(heatmap.get_yticklabels(), fontsize=30,rotation=360)\n",
    "\n",
    "plt.show();\n",
    "#fig.savefig(f\"image/상관계수히트맵\", transparent = True,dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce541e5a",
   "metadata": {},
   "source": [
    "## 다중공산성 확인하기"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f92feef",
   "metadata": {},
   "source": [
    "### VIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66810553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T03:37:11.569633Z",
     "start_time": "2023-05-31T03:37:11.555680Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "변수 제거: VIF 값이 10 이상인 변수 제거. \n",
    "\n",
    "변수 변환: 다중공선성을 완화하기 위해 변수를 변환-> 표준화,로그 변환\n",
    "\n",
    "변수 결합: 다중공선성이 있는 변수들을 결합하여 새로운 변수를 생성. \n",
    "\n",
    "주성분 분석(PCA): PCA를 사용하여 다중공선성이 있는 변수들을 주성분으로 변환하여 변수들 간의 선형 종속성을 최소화\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faaca7dc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T03:37:32.423602Z",
     "start_time": "2023-05-31T03:37:28.472746Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1. 특정 독립 변수를 종속 변수로 선형 회귀 모델을 적합\n",
    "2. 이때 해당 독립 변수를 제외한 나머지 독립 변수들을 독립 변수로 사용하여 선형 회귀 모델을 다시 적합\n",
    "3. 두 번째 회귀 모델의 결정 계수(R-squared)를 구합니다.\n",
    "4. VIF 값은 1 / (1 - R-squared)으로 계산됩니다.\n",
    "\n",
    "* VIF < 1 : 다른 독립 변수들과의 상관 관계가 크지 않음 = 다중곤선성의 영향 적음.\n",
    "* VIF == 1 :독립 변수 간의 상관 관계가 약하거나 전혀 없음.\n",
    "* VIF > 1  :다른 독립 변수들과의 상관 관계가 강력 =  다중공선성의 영향 높음.\n",
    "\"\"\"\n",
    "\n",
    "df=load_dataset3(1, 0, 1)[0]\n",
    "#df=pd.concat([tmp[0],tmp[2]],axis=1)\n",
    "df.columns\n",
    "df['Intercept'] = 1\n",
    "\n",
    "vif_df = pd.DataFrame()\n",
    "vif_df['Features'] = df.columns\n",
    "vif_df['VIF'] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "\n",
    "vif_df\n",
    "high_vif_vars = vif_df[vif_df['VIF'] >= 10]['Features']\n",
    "low_vif_vars = vif_df[vif_df['VIF'] < 10]['Features']\n",
    "\n",
    "high_vif_vars=[i for i in high_vif_vars]\n",
    "\n",
    "print(\"Variables with VIF >= 10:\")\n",
    "print(high_vif_vars)\n",
    "\n",
    "print(\"Variables with VIF < 10:\")\n",
    "print(low_vif_vars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f745d033",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "87cc966e",
   "metadata": {},
   "source": [
    "## Model selelct(automl-lazypredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add0591d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:35:06.549111Z",
     "start_time": "2023-05-31T04:35:06.534057Z"
    }
   },
   "outputs": [],
   "source": [
    "#for idx,i in enumerate(lazypredict.Supervised.REGRESSORS):\n",
    "#    print(idx,i)\n",
    "reg_list=[]\n",
    "for idx,i in enumerate(lazypredict.Supervised.REGRESSORS):\n",
    "    if(idx in [33,40,41]):\n",
    "        reg_list.append(i)\n",
    "reg_list.append(('CatBoostRegressor',CatBoostRegressor))\n",
    "lazypredict.Supervised.REGRESSORS=reg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4afa471",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:40:39.124869Z",
     "start_time": "2023-05-31T04:36:12.616973Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath='../result/base_model/'\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    base_set=load_dataset(1,0,0) # x_train,x_test,y_train,y_test\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(base_set[0], base_set[2], test_size=0.33, random_state=624)\n",
    "    model = LazyRegressor(verbose=1, ignore_warnings=True)\n",
    "    pred,result= model.fit(X_train, X_valid, y_train, y_valid)\n",
    "    result.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26e9064a",
   "metadata": {},
   "source": [
    "# Select version\n",
    "\n",
    "basemodel <br>\n",
    "\n",
    "version 1 <br>\n",
    " - 10분간 유량 변화량, 방류량-유입량, 월, 일 <br>\n",
    "\n",
    " version 2 <br>\n",
    " - 유량/수위 pca, 방류량/유입량 pca, 저수량,수위 pca, 월, 일 <br>\n",
    "\n",
    " version 3 <br>\n",
    " - 유량 있는 채로 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a1f111",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:49:13.965558Z",
     "start_time": "2023-05-31T04:49:13.948623Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#! version1 = 10분간 유량 변화량 추가\n",
    "#! version2 = pca 이용\n",
    "#! version3 = 유량원본 이용 \n",
    "\n",
    "model_scores=[]\n",
    "#version,defver=\"version1\",load_dataset \n",
    "version,defver=\"version2\",load_dataset2\n",
    "#version,defver=\"version3\",load_dataset3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8a75bf9d",
   "metadata": {},
   "source": [
    "## Base model\n",
    "\n",
    "columns=['wl_js_br', 'wl_cd_br', 'wl_hj_br', 'wl_jn_br', 'wl_pd_dam', <br>\n",
    "       'inf_pd_dam', 'sfw_pd_dam', 'ecpc_pd_dam', 'tototf_pd_dam', 'pr_jg',<br>\n",
    "       'pr_dg', 'pr_sj', 'tl_gh_br']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98091535",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:49:15.727907Z",
     "start_time": "2023-05-31T04:49:15.545792Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath='../result/base_model/'\n",
    "\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    base_set=load_dataset(1,0,0)\n",
    "    base_set[0].columns\n",
    "    model = RandomForestRegressor(random_state=624,n_jobs=-1)\n",
    "    pred=model.fit(base_set[0], base_set[2])\n",
    "    pred=model.predict(base_set[1])\n",
    "    pred=pd.DataFrame(pred,columns=['wl_jamsu'])\n",
    "    pred=pred[:-1]\n",
    "    pred.to_csv(f\"{filepath}preds.csv\",index=False)\n",
    "    joblib.dump(model, f\"{filepath}wl_jamsu.pkl\")\n",
    "    fig=scatter_plot(answer,pred)\n",
    "    fig.savefig(f\"../image/베이스라인pred\", transparent = True,dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    pred=pd.read_csv(f\"{filepath}preds.csv\")\n",
    "    model_scores.append(metric(answer,pred)[0])\n",
    "    fig=scatter_plot(answer,pred) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f938adf",
   "metadata": {},
   "source": [
    "## Feature engineering model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260987de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:49:17.054764Z",
     "start_time": "2023-05-31T04:49:16.873164Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath=f'../result/{version}/model/'\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    base_set=defver(1,0,1)\n",
    "    base_set[0].columns\n",
    "    model = RandomForestRegressor(random_state=624,n_jobs=-1)\n",
    "    pred=model.fit(base_set[0], base_set[2])\n",
    "    pred=model.predict(base_set[1])\n",
    "    pred=pd.DataFrame(pred,columns=['wl_jamsu'])\n",
    "    pred=pred[:-1]\n",
    "    pred.to_csv(f\"{filepath}preds.csv\",index=False)\n",
    "    #joblib.dump(model, f\"{filepath}wl_jamsu.pkl\")\n",
    "    fig=scatter_plot(answer,pred)\n",
    "    #fig.savefig(f\"image/엔지니어pred\", transparent = True,dpi=300, bbox_inches='tight')\n",
    "else:\n",
    "    pred=pd.read_csv(f\"{filepath}preds.csv\")\n",
    "    model_scores.append(metric(answer,pred)[0])\n",
    "    fig=scatter_plot(answer,pred) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "91e76383",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-31T04:50:37.402805Z",
     "start_time": "2023-05-31T04:50:37.381875Z"
    }
   },
   "source": [
    "## moving average=6 /Feature engineering model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8313b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=f'../result/{version}/model2/'\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    base_set=defver(1,6,1)\n",
    "    base_set[0].columns\n",
    "    model = RandomForestRegressor(random_state=624,n_jobs=-1)\n",
    "    pred=model.fit(base_set[0], base_set[2])\n",
    "    pred=model.predict(base_set[1])\n",
    "    pred=pd.DataFrame(pred,columns=['wl_jamsu'])\n",
    "    pred=pred[:-1]\n",
    "    pred.to_csv(f\"{filepath}preds.csv\",index=False)\n",
    "    fig=scatter_plot(answer,pred)\n",
    "else:\n",
    "    pred=pd.read_csv(f\"{filepath}preds.csv\")\n",
    "    model_scores.append(metric(answer,pred)[0])\n",
    "    fig=scatter_plot(answer,pred) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a6aa845",
   "metadata": {},
   "source": [
    "## moving average=6 /Feature engineering model / hyper paramete optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4017047e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T06:37:37.540413Z",
     "start_time": "2023-05-18T06:37:37.272036Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "filepath=f'../result/{version}/model3/'\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    base_set=defver(1,6,1)\n",
    "    pred,model=create_model(filepath,100,base_set)\n",
    "    pred.to_csv(f\"{filepath}preds.csv\",index=False)\n",
    "    joblib.dump(model, f\"{filepath}optimized_wl_jamsu.pkl\") \n",
    "    fig=scatter_plot(answer,pred)\n",
    "else:\n",
    "    pred=pd.read_csv(f\"{filepath}preds.csv\")\n",
    "    fig=scatter_plot(answer,pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6a87269",
   "metadata": {},
   "source": [
    "# Result ( by model with lead time)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2401d98",
   "metadata": {},
   "source": [
    "##  base model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6949cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T07:45:10.891867Z",
     "start_time": "2023-05-20T07:45:10.758847Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath = f\"../result/base_model/leadtime/\" # basic\n",
    "\n",
    "preds_list,scores=[],[]\n",
    "days_list=[1,3,6,9,12,18,24,30,36,48,60,72]\n",
    "\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        print(\"#\"*50)\n",
    "        print(\"LeadTime=%d\"%days_list[i])\n",
    "        print(\"Modeling Start\")\n",
    "        base_set=load_dataset(days_list[i],0,0)\n",
    "        model=RandomForestRegressor(n_jobs=-1,random_state=624)\n",
    "        model = model.fit(base_set[0],base_set[2])\n",
    "        pred = model.predict(base_set[1])\n",
    "        pred=pd.DataFrame(pred,columns=['wl_jamsu'])\n",
    "        pred=pred[:-days_list[i]]\n",
    "        print(\"RMSE score is %f / NSE score is %f\"%(metric(answer,pred)[0],metric(answer,pred)[1]))\n",
    "        preds_list.append(pred)\n",
    "        pred.to_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\",index=False)\n",
    "        #joblib.dump(model, f\"D:/{filepath}wl_jamsu({days_list[i]}).pkl\")       \n",
    "        print(\"Modeling END\")\n",
    "        \n",
    "        del base_set\n",
    "        del model\n",
    "else:\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        tmp=pd.read_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\")\n",
    "        #preds_list.append(tmp)\n",
    "        scores.append(metric(answer,tmp)[0])\n",
    "        #print(\"#\"*50)\n",
    "        #print(\"LeadTime=%d\"%days_list[i])\n",
    "        print(\"RMSE score is %f / NSE score is %f\"%(metric(answer,tmp)[0],metric(answer,tmp)[1]))\n",
    "        #print(\"#\"*50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfd45528",
   "metadata": {},
   "source": [
    "## Feature engineering model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e3b527",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = f\"../result/{version}/model/leadtime/\" # feature\n",
    "\n",
    "preds_list,scores2=[],[]\n",
    "days_list=[1,3,6,9,12,18,24,30,36,48,60,72]\n",
    "\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        print(\"#\"*50)\n",
    "        print(\"LeadTime=%d\"%days_list[i])\n",
    "        print(\"Modeling Start\")\n",
    "        base_set=defver(days_list[i],0,1)\n",
    "        model=RandomForestRegressor(n_jobs=-1,random_state=624)\n",
    "        model = model.fit(base_set[0],base_set[2])\n",
    "        pred = model.predict(base_set[1])\n",
    "        pred=pd.DataFrame(pred,columns=['wl_jamsu'])\n",
    "        pred=pred[:-days_list[i]]\n",
    "        print(\"RMSE score is %f / NSE score is %f\"%(metric(answer,pred)[0],metric(answer,pred)[1]))\n",
    "        preds_list.append(pred)\n",
    "        pred.to_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\",index=False)\n",
    "        #joblib.dump(model, f\"D:/{filepath}wl_jamsu({days_list[i]}).pkl\")       \n",
    "        print(\"Modeling END\")\n",
    "        \n",
    "        del base_set\n",
    "        del model\n",
    "else:\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        tmp=pd.read_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\")\n",
    "        preds_list.append(tmp)\n",
    "        scores2.append(metric(answer,tmp)[0])\n",
    "        #print(\"#\"*50)\n",
    "        #print(\"LeadTime=%d\"%days_list[i])\n",
    "        #print(\"RMSE score is %f / NSE score is %f\"%(metric(answer,tmp)[0],metric(answer,tmp)[1]))\n",
    "        #print(\"#\"*50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a08266e",
   "metadata": {},
   "source": [
    "## moving_average=6 / feature engineering model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f70fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath =f\"../result/{version}/model2/leadtime/\" # feature\n",
    "\n",
    "preds_list,scores3=[],[]\n",
    "days_list=[1,3,6,9,12,18,24,30,36,48,60,72]\n",
    "#basic_preds_list=[]\n",
    "\n",
    "if(check(filepath)==0): # 해당경로에 없을 때\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        print(\"#\"*50)\n",
    "        print(\"LeadTime=%d\"%days_list[i])\n",
    "        print(\"Modeling Start\")\n",
    "        base_set=defver(days_list[i],6,1)\n",
    "        model=RandomForestRegressor(n_jobs=-1,random_state=624)\n",
    "        model = model.fit(base_set[0],base_set[2])\n",
    "        pred = model.predict(base_set[1])\n",
    "        pred=pd.DataFrame(pred,columns=['wl_jamsu'])\n",
    "        pred=pred[:-days_list[i]]\n",
    "        print(\"RMSE score is %f / NSE score is %f\"%(metric(answer,pred)[0],metric(answer,pred)[1]))\n",
    "        preds_list.append(pred)\n",
    "        pred.to_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\",index=False)    \n",
    "        print(\"Modeling END\")\n",
    "        \n",
    "        del base_set\n",
    "        del model\n",
    "else:\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        tmp=pd.read_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\")\n",
    "        #preds_list.append(tmp)\n",
    "        scores3.append(metric(answer,tmp)[0])\n",
    "        #print(\"#\"*50)\n",
    "        #print(\"LeadTime=%d\"%days_list[i])\n",
    "        #print(\"RMSE score is %f / NSE score is %f\"%(metric(answer,tmp)[0],metric(answer,tmp)[1]))\n",
    "        #print(\"#\"*50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d4735b52",
   "metadata": {},
   "source": [
    "## moving_average=6 / feature engineering / optimized model result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d95acf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-18T05:58:02.017734Z",
     "start_time": "2023-05-18T05:58:01.866273Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath =f\"../result/{version}/model3/leadtime/\" # feature\n",
    "days_list=[1,3,6,9,12,18,24,30,36,48,60,72]\n",
    "preds_list,scores4=[],[]\n",
    "\n",
    "if(check(filepath)==0):\n",
    "    tmp = joblib.load(f\"../result/{version}/model3/optimized_wl_jamsu.pkl\")\n",
    "    params = tmp.best_trial.params\n",
    "    del tmp\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        print(\"#\"*50)\n",
    "        print(\"LeadTime=%d\"%days_list[i])\n",
    "        print(\"Modeling Start\")\n",
    "        base_set=defver(days_list[i],6,1)\n",
    "        model=RandomForestRegressor(**params,random_state=624,n_jobs=-1)\n",
    "        model = model.fit(base_set[0],base_set[2])\n",
    "        pred = model.predict(base_set[1])\n",
    "        pred=pd.DataFrame(pred,columns=['wl_jamsu'])\n",
    "        pred=pred[:-days_list[i]]\n",
    "        print(\"RMSE score is %f / NSE score is %f\"%(metric(answer,pred)[0],metric(answer,pred)[1]))\n",
    "        preds_list.append(pred)\n",
    "        pred.to_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\",index=False)\n",
    "        if(version==\"version2\"):\n",
    "            joblib.dump(model, f\"D:/waterdata/result/leadtime/wl_jamsu({days_list[i]}).pkl\")       \n",
    "        print(\"Modeling END\")\n",
    "        del base_set\n",
    "        del model\n",
    "else:\n",
    "    for i in tqdm(range(len(days_list))):\n",
    "        tmp=pd.read_csv(f\"{filepath}wl_jamsu({days_list[i]}).csv\")\n",
    "        preds_list.append(tmp)\n",
    "        scores4.append(metric(answer,tmp)[0])\n",
    "        #print(\"#\"*50)\n",
    "        #print(\"LeadTime=%d\"%days_list[i])\n",
    "        print(\"RMSE score is %f \"%(metric(answer,tmp)[0]))\n",
    "        #print(\"#\"*50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea69c24b",
   "metadata": {},
   "source": [
    "# Visaulize Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f90bb8ec",
   "metadata": {},
   "source": [
    "### 모델별 선행시간에 따른 rmse score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb74a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores1=[] #base\n",
    "\n",
    "scores2=[] #base+featureengu\n",
    "scores3=[] #abe+feate+ma\n",
    "scores4=[] #base+featureengu\n",
    "\n",
    "scores5=[] #abe+feate+ma\n",
    "scores6=[] #base+featureengu\n",
    "scores7=[] #abe+feate+ma\n",
    "\n",
    "scores8=[] #abe+feate+ma\n",
    "scores9=[] #base+featureengu\n",
    "scores10=[] #abe+feate+ma\n",
    "\n",
    "for i in tqdm(range(len(days_list))):\n",
    "    tmp=pd.read_csv(f\"../result/base_model/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores1.append(metric(answer,tmp)[0])\n",
    "    ##################################################################################\n",
    "    tmp=pd.read_csv(f\"../result/version1/model/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores2.append(metric(answer,tmp)[0])\n",
    "    \n",
    "    tmp=pd.read_csv(f\"../result/version1/model2/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores3.append(metric(answer,tmp)[0])\n",
    "    \n",
    "    tmp=pd.read_csv(f\"../result/version1/model3/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores4.append(metric(answer,tmp)[0])\n",
    "    ##################################################################################\n",
    "    tmp=pd.read_csv(f\"../result/version2/model/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores5.append(metric(answer,tmp)[0])\n",
    "    \n",
    "    tmp=pd.read_csv(f\"../result/version2/model2/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores6.append(metric(answer,tmp)[0])\n",
    "    \n",
    "    tmp=pd.read_csv(f\"../result/version2/model3/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores7.append(metric(answer,tmp)[0])\n",
    "    ##################################################################################\n",
    "    tmp=pd.read_csv(f\"../result/version3/model/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores8.append(metric(answer,tmp)[0])\n",
    "    \n",
    "    tmp=pd.read_csv(f\"../result/version3/model2/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores9.append(metric(answer,tmp)[0])\n",
    "    \n",
    "    tmp=pd.read_csv(f\"../result/version3/model3/leadtime/wl_jamsu({days_list[i]}).csv\")\n",
    "    scores10.append(metric(answer,tmp)[0])\n",
    "    ##################################################################################\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7cfee2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-20T07:45:40.241046Z",
     "start_time": "2023-05-20T07:45:40.060943Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(10,7))\n",
    "plt.scatter(days_list[:-1], scores[:-1], s=70, label='basic',marker='x')\n",
    "\n",
    "#plt.scatter(days_list[:-1], scores2[:-1], s=70, label='version 1 + engineered',marker='o')\n",
    "#plt.scatter(days_list[:-1], scores3[:-1], s=70, label='version 1 + engineered + ma6',marker='o')\n",
    "#plt.scatter(days_list[:-1], scores4[:-1], s=70, label='version 1 + engineered + ma6 + optimized',marker='o')\n",
    "\n",
    "#plt.scatter(days_list[:-1], scores5[:-1], s=70, label='version 2 + engineered',marker='*')\n",
    "#plt.scatter(days_list[:-1], scores6[:-1], s=70, label='version 2 + engineered + ma6',marker='*')\n",
    "plt.scatter(days_list[:-1], scores7[:-1], s=70, label='version 2 + engineered + ma6 + optimized',marker='*')\n",
    "\n",
    "#plt.scatter(days_list[:-1], scores8[:-1], s=70, label='version 3 + engineered',marker='^')\n",
    "#plt.scatter(days_list[:-1], scores9[:-1], s=70, label='version 3 + engineered + ma6',marker='^')\n",
    "plt.scatter(days_list[:-1], scores10[:-1], s=70, label='version 3 + engineered + ma6 + optimized',marker='^')\n",
    "\n",
    "\n",
    "plt.legend(fontsize=15)\n",
    "plt.xticks(days_list[:-1], fontsize=10)\n",
    "plt.yticks(fontsize=15)\n",
    "\n",
    "plt.xlabel(\"lead time\", fontsize=20)\n",
    "plt.ylabel(\"RMSE(m)\", fontsize=20);\n",
    "\n",
    "#fig.savefig(f\"image/model comparison\", transparent = True,dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59bf6156",
   "metadata": {},
   "source": [
    "## leadtime result by model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd8ae21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T05:21:57.452340Z",
     "start_time": "2023-05-16T05:21:57.138848Z"
    }
   },
   "outputs": [],
   "source": [
    "base_set=load_dataset(1,0,0)\n",
    "ymdhm=base_set[1].index[1:]\n",
    "ymdhm=pd.to_datetime(ymdhm)\n",
    "ymdhm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cf6760c",
   "metadata": {},
   "source": [
    "### model 3 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43aa9025",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:24:24.476958Z",
     "start_time": "2023-05-15T09:24:23.760063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig=plt.figure(figsize=(30,20))\n",
    "plt.title(\"선행시간별예측\", fontsize=40)\n",
    "plt.plot(ymdhm,preds_list[0]) # 선행시간 10m\n",
    "plt.plot(ymdhm,preds_list[1]) # 선행시간 30m\n",
    "plt.plot(ymdhm,preds_list[2]) # 선행시간 60m\n",
    "plt.plot(ymdhm,preds_list[3]) # 선행시간 90m\n",
    "plt.plot(ymdhm,preds_list[4]) # 선행시간 120m\n",
    "plt.plot(ymdhm,preds_list[5]) # 선행시간 180m\n",
    "plt.plot(ymdhm,preds_list[6]) # 선행시간 360m\n",
    "plt.plot(ymdhm,preds_list[7]) # 선행시간 480m\n",
    "plt.plot(ymdhm,preds_list[8]) # 선행시간 480m\n",
    "plt.plot(ymdhm,preds_list[9]) # 선행시간 480m\n",
    "plt.plot(ymdhm,answer,color='black')\n",
    "plt.xticks(fontsize= 30)\n",
    "plt.yticks(fontsize= 25)\n",
    "plt.ylabel(\"수위\",fontsize= 25)\n",
    "plt.legend(['10','30','60','90','120','180','240','360','480','600','answer'],fontsize=20)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849fd7bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T05:21:54.755684Z",
     "start_time": "2023-05-16T05:21:54.752050Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.rcParams['axes.labelsize'] = 65\n",
    "plt.rcParams['xtick.labelsize'] = 65\n",
    "plt.rcParams['ytick.labelsize'] = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef33a7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T05:22:31.201840Z",
     "start_time": "2023-05-16T05:21:59.162357Z"
    },
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(5, 2, figsize=(116, 80))\n",
    "days_list=[1,3,6,9,12,18,24,30,36,48,60]\n",
    "\n",
    "predslist=preds_list\n",
    "\n",
    "colors = {'prediction': 'blue', 'observation': 'black', 'red': 'red', 'green': 'green'}\n",
    "k = [0, 1 ,2, 3, 4, 5, 6, 7, 8, 9]\n",
    "titles = ['(a) 10 min.', '(b) 30 min.','(c) 60 min.', '(d) 90 min.','(e) 120 min.', \n",
    "          '(f) 180 min.','(g) 240 min.', '(h) 360 min.','(i) 480 min.','(j) 600 min.']\n",
    "\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    plot_rmse(ax, answer, predslist[k[i]], label='RMSE')\n",
    "    ax.plot([], [], color=colors['red'], marker='o', linestyle='None', markersize=20)\n",
    "    ax.plot([], [], color=colors['green'], marker='o', linestyle='None', markersize=30)\n",
    "    ax.plot(ymdhm, predslist[k[i]], color=colors['prediction'])\n",
    "    ax.plot(ymdhm, answer, color=colors['observation'])\n",
    "    ax.set_title(titles[i], fontsize=95, loc='left', fontdict={'color': 'black'})\n",
    "    ax.set(xlabel=\"2022\",ylabel='Water level(cm)')\n",
    "    \n",
    "    # 날짜형식을 문자로 변경\n",
    "    date_fmt = '%d %b'  \n",
    "    date_formatter = mdates.DateFormatter(date_fmt)\n",
    "    ax.xaxis.set_major_formatter(date_formatter)\n",
    "    # subplot형태로 만들기위해 다른 label 없앰 \n",
    "    \n",
    "    # 중간에 수평선 그리기 \n",
    "    ax.axhline(y=550, color='gray', linestyle='--', linewidth=4)\n",
    "    ax.axhline(y=620, color='gray', linestyle='--', linewidth=4)\n",
    "    \n",
    "    ax.set_yticks([250, 400, 550, 620, 800,1000])\n",
    "    ax.tick_params(axis='both', which='major', width=8, colors='black',labelsize=65, length=18, direction=\"out\", bottom=True, left=True,pad=30)\n",
    "    ax.label_outer()\n",
    "\n",
    "    \n",
    "    red_count,green_count = 0,0\n",
    "    for j, (obs, pred) in enumerate(zip(answer['wl_js_br'], predslist[k[i]]['wl_jamsu'])):\n",
    "        if obs >= 550 and pred < 550:\n",
    "            ax.plot(ymdhm[j], pred, color=colors['red'], marker='o', markersize=20)\n",
    "            red_count += 1\n",
    "        elif obs < 550 and pred >= 550:\n",
    "            ax.plot(ymdhm[j], pred, color=colors['green'], marker='o', markersize=20)\n",
    "            green_count += 1\n",
    "        \n",
    "    # 마지막차트에는 legend 2개가 더 있어야 함        \n",
    "    if(i==9):        \n",
    "        ax.legend([f'False-Negative: {red_count}', f'False-Positive: {green_count}','Prediction', 'Observation'],fontsize=65)\n",
    "    else:        \n",
    "        ax.legend([f'False-Negative: {red_count}', f'False-Positive: {green_count}'],fontsize=65,loc='upper left')\n",
    "        \n",
    "    # 테두리 두껍게 안하면 안보임.\n",
    "    ax.spines['top'].set_linewidth(4)\n",
    "    ax.spines['bottom'].set_linewidth(4)\n",
    "    ax.spines['left'].set_linewidth(4)\n",
    "    ax.spines['right'].set_linewidth(4)\n",
    "fig;\n",
    "\n",
    "\n",
    "#fig.savefig(f\"image/모델3 선행시간별그래프\", transparent = True,dpi=300, bbox_inches='tight');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(5, 2, figsize=(12, 20))\n",
    "titles = ['(a) 10 min.', '(b) 30 min.','(c) 60 min.', '(d) 90 min.','(e) 120 min.', \n",
    "          '(f) 180 min.','(g) 240 min.', '(h) 360 min.','(i) 480 min.','(j) 600 min.']\n",
    "\n",
    "for i in range(min(len(preds_list), 10)):  # 최대 10개의 그림을 출력하도록 수정\n",
    "    predict = preds_list[i]\n",
    "    \n",
    "    predict['tmp'] = predict['wl_jamsu'] > 550\n",
    "    answer['tmp'] = answer['wl_js_br'] > 550\n",
    "\n",
    "    cm = confusion_matrix(answer['tmp'], predict['tmp'])\n",
    "    labels = ['0', '1']\n",
    "\n",
    "    ax = axes[i // 2, i % 2]  # 서브플롯 인덱스 설정\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
    "    ax.set_xlabel('Predicted' if i == min(len(preds_list), 10) - 1 else '')\n",
    "    ax.set_ylabel('Actual' if i == min(len(preds_list), 10) - 1 else '')\n",
    "    ax.set_title(f'{titles[i]}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2d0787df",
   "metadata": {},
   "source": [
    "# Permutation importacne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a557d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_set=load_dataset(days_list[i],6,1)\n",
    "feature = base_set[0].columns\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c86139",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-16T05:24:50.772021Z",
     "start_time": "2023-05-16T05:24:50.755633Z"
    }
   },
   "outputs": [],
   "source": [
    "filepath=\"result/leadtime2/leadtime\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd9bb3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-15T09:42:17.554237Z",
     "start_time": "2023-05-15T09:42:04.927514Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(days_list)):\n",
    "    tmp_model=joblib.load(f\"D:/waterdata/result/leadtime/wl_jamsu({days_list[i]}).pkl\")\n",
    "    importance = tmp_model.feature_importances_\n",
    "    del tmp_model\n",
    "    base_set=load_dataset(days_list[i],6,1)\n",
    "    feature = base_set[0].columns\n",
    "    importances = pd.DataFrame()\n",
    "    importances['feature'] = base_set[0].columns\n",
    "    importances['importances'] = importance\n",
    "    importances.sort_values('importances', ascending=False, inplace=True)\n",
    "    importances.reset_index(drop=True, inplace=True)\n",
    "    fig=plt.figure(figsize=(8,6))\n",
    "    sns.barplot(x='importances', y='feature', data=importances[:11], color='royalblue')\n",
    "    plt.title('%s Leadtime Feature Importances(1~10)'%days_list[i], fontsize=18)\n",
    "    fig.savefig(f\"../image/importance/Importance({days_list[i]}).png\", transparent = True,dpi=300, bbox_inches='tight')\n",
    "    fig;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf5b97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "water_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "797.766px",
    "left": "461.969px",
    "top": "132.438px",
    "width": "240.375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 680.844,
   "position": {
    "height": "40px",
    "left": "1335.3px",
    "right": "20px",
    "top": "183.969px",
    "width": "582px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
